# Toy Model é¡¹ç›®è§„èŒƒ

## ğŸ“‹ æ¦‚è¿°

Toy Modelæ˜¯ç”¨äºéªŒè¯PPOè·¯ç”±ç­–ç•¥çš„M/M/1é˜Ÿåˆ—ä»¿çœŸå®éªŒï¼Œ**å®Œå…¨ç‹¬ç«‹äºVidurä¸»é¡¹ç›®**ï¼Œä½†å¯é€‰æ‹©æ€§å¤ç”¨æ ¸å¿ƒPPOç»„ä»¶ã€‚

---

## ğŸ¯ æ ¸å¿ƒåŸåˆ™

### 1. æ¨¡å—éš”ç¦»åŸåˆ™
- **Toy Modelä»£ç **: ç»Ÿä¸€æ”¾åœ¨ `toymodel/` æ ¹ç›®å½•
- **Vidurä»£ç **: ä¿æŒåœ¨ `vidur/` å’Œ `src/` ä¸­
- **ç¦æ­¢äº¤å‰ä¾èµ–**: Toy modelä¸åº”è¢«Vidurä¾èµ–

### 2. å¤ç”¨ç­–ç•¥
| ç»„ä»¶ç±»å‹ | å¤ç”¨æ–¹å¼ | è¯´æ˜ |
|---------|---------|------|
| PPOç®—æ³•æ ¸å¿ƒ | âœ… ç›´æ¥import `src.core.algorithms.PPOTrainer` | ç®—æ³•é€šç”¨ |
| ActorCriticç½‘ç»œ | âœ… ç›´æ¥import `src.core.models.ActorCritic` | ç½‘ç»œé€šç”¨ |
| çŠ¶æ€æ„å»º | âŒ ç‹¬ç«‹å®ç° `toymodel.state_builder` | çŠ¶æ€ç©ºé—´ä¸åŒ |
| å¥–åŠ±è®¡ç®— | âŒ ç‹¬ç«‹å®ç° `toymodel.reward` | å¥–åŠ±å‡½æ•°ä¸åŒ |
| ä»¿çœŸç¯å¢ƒ | âŒ ç‹¬ç«‹å®ç° `toymodel.environment` | M/M/1 vs äº‹ä»¶é©±åŠ¨ |

### 3. æ¥å£è®¾è®¡åŸåˆ™
- **ç®€æ´ä¼˜å…ˆ**: Toy modelçŠ¶æ€ç»´åº¦ << VidurçŠ¶æ€ç»´åº¦
- **å¯æµ‹è¯•æ€§**: æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å¯æµ‹
- **æ˜¾å¼éªŒè¯**: ç¦æ­¢fallbackï¼Œé…ç½®ç¼ºå¤±ç›´æ¥æŠ¥é”™

---

## ğŸ“ ç›®å½•ç»“æ„è§„èŒƒ

```
Vidur_toymodel/
â”œâ”€â”€ toymodel/                    # Toy Modelæ ¹ç›®å½• (ç‹¬ç«‹æ¨¡å—)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py           # M/M/1é˜Ÿåˆ—ç¯å¢ƒ
â”‚   â”œâ”€â”€ state_builder.py         # çŠ¶æ€æ„å»º
â”‚   â”œâ”€â”€ reward.py                # å¥–åŠ±è®¡ç®—
â”‚   â”œâ”€â”€ schedulers/              # è·¯ç”±ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ppo_scheduler.py    # PPOè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ oracle.py            # æœ€ä¼˜ç­–ç•¥
â”‚   â”‚   â””â”€â”€ baselines.py         # Random/RR
â”‚   â”œâ”€â”€ training/                # è®­ç»ƒæµç¨‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ trainer.py           # ä¸»è®­ç»ƒå™¨
â”‚   â””â”€â”€ metrics/                 # æŒ‡æ ‡æ”¶é›†
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ collector.py
â”‚
â”œâ”€â”€ configs/toymodel/            # Toy Modelé…ç½®
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ balanced_load.yaml
â”‚   â””â”€â”€ high_load.yaml
â”‚
â”œâ”€â”€ demo/                        # ç¤ºä¾‹ä»£ç 
â”‚   â””â”€â”€ demo_environment.py      # ç¯å¢ƒä½¿ç”¨æ¼”ç¤º
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ toymodel_train.sh        # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ toymodel_eval.sh         # è¯„ä¼°è„šæœ¬
â”‚
â”œâ”€â”€ tests/toymodel/              # Toy Modelæµ‹è¯•
â”‚   â”œâ”€â”€ test_environment.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ tmp/                         # ä¸´æ—¶æµ‹è¯•æ–‡ä»¶ (å³æµ‹å³åˆ )
â”‚   â””â”€â”€ README.md                # ä»…ä¿ç•™README
â”‚
â”œâ”€â”€ outputs/toymodel/            # Toy Modelè¾“å‡º
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ tensorboard/
â”‚
â”œâ”€â”€ .claude/
â”‚   â””â”€â”€ TOYMODEL.md             # æœ¬æ–‡æ¡£ (å¿…è¯»)
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ toymodel_ppo_routing_design.md  # æŠ€æœ¯æ–¹æ¡ˆ
    â””â”€â”€ toymodel_implementation.md      # å®ç°æ–‡æ¡£
```

**å…³é”®ç‚¹**:
- âœ… Toy modelä»£ç åœ¨ `toymodel/` æ ¹ç›®å½•ï¼Œä¸ `vidur/` å’Œ `src/` å¹³çº§
- âœ… é…ç½®/è„šæœ¬/æµ‹è¯•/è¾“å‡ºéƒ½æŒ‰ `toymodel/` å‘½åç©ºé—´ç»„ç»‡
- âœ… ä¸´æ—¶æµ‹è¯•æ–‡ä»¶ç»Ÿä¸€æ”¾åœ¨ `tmp/`ï¼Œç”¨å®Œå³åˆ 
- âŒ ä¸åœ¨ `src/toymodel/` ä¸‹ï¼Œé¿å…ä¸Viduræ ¸å¿ƒä»£ç æ··æ·†

---

## ğŸ”§ ç»„ä»¶å¤ç”¨æ¥å£

### å¤ç”¨PPOç»„ä»¶ç¤ºä¾‹

```python
# toymodel/training/trainer.py

from src.core.models import ActorCritic
from src.core.algorithms import PPOTrainer

# ç›´æ¥ä½¿ç”¨ï¼Œä»…è°ƒæ•´ç»´åº¦å’Œè¶…å‚æ•°
policy = ActorCritic(
    state_dim=6,              # Toy modelç®€åŒ–çŠ¶æ€
    action_dim=2,             # 2ä¸ªreplica
    hidden_size=64,           # æ¯”Viduræ›´å°
    layer_N=1,
    gru_layers=1,
    enable_decoupled=False,   # ä½¿ç”¨æ ‡å‡†æ¶æ„
)

trainer = PPOTrainer(
    policy=policy,
    lr=3e-4,
    clip_ratio=0.2,
    minibatch_size=32,        # æ¯”Viduræ›´å°
)
```

### ç‹¬ç«‹å®ç°çŠ¶æ€æ„å»º

```python
# toymodel/state_builder.py

import numpy as np

class ToyStateBuilder:
    """Toy modelçŠ¶æ€æ„å»ºå™¨ (6ç»´çŠ¶æ€)."""

    def build_state(
        self,
        replica_queues: list[int],        # [q1, q2]
        replica_utilizations: list[float], # [u1, u2]
        current_request_type: int,         # 0 or 1
        time_since_last_arrival: float,
    ) -> np.ndarray:
        """æ„å»ºçŠ¶æ€å‘é‡ [q1, u1, q2, u2, type, time]."""
        return np.array([
            replica_queues[0],
            replica_utilizations[0],
            replica_queues[1],
            replica_utilizations[1],
            float(current_request_type),
            time_since_last_arrival,
        ], dtype=np.float32)
```

---

## ğŸš« å‘½åè§„èŒƒ

### ç¦ç”¨å‰ç¼€/åç¼€
- âŒ `toymodel_enhanced_*`, `*_toymodel_v2`
- âŒ `simple_*`, `toy_*_simple` (é¿å…è´¬ä½æ€§å‘½å)

### æ¨èå‘½å
- âœ… `toymodel/environment.py` (æ¸…æ™°æ¨¡å—å)
- âœ… `toymodel/schedulers/oracle.py` (åŠŸèƒ½å¯¼å‘)
- âœ… `configs/toymodel/balanced_load.yaml` (åœºæ™¯å¯¼å‘)

---

## âœ… é…ç½®è§„èŒƒ

### é…ç½®æ–‡ä»¶ç»“æ„

```yaml
# configs/toymodel/base.yaml

environment:
  num_replicas: 2
  service_rates:
    replica_0: {type_A: 10.0, type_B: 5.0}
    replica_1: {type_A: 5.0, type_B: 10.0}
  arrival_rates: {type_A: 6.0, type_B: 6.0}
  max_steps: 10000
  seed: 42

model:
  state_dim: 6        # å›ºå®šç»´åº¦
  action_dim: 2       # å›ºå®šä¸º2ä¸ªreplica
  hidden_size: 64
  layer_N: 1
  gru_layers: 1

ppo:
  learning_rate: 0.0003
  gamma: 0.99
  clip_ratio: 0.2
  entropy_coef: 0.01
  minibatch_size: 32
  rollout_length: 2048

reward:
  queue_weight: 1.0
  routing_bonus: 0.1

training:
  total_steps: 100000
  eval_interval: 1000
  checkpoint_interval: 5000
```

**é…ç½®éªŒè¯**: å¯åŠ¨æ—¶å¿…é¡»æ˜¾å¼éªŒè¯æ‰€æœ‰å¿…éœ€å­—æ®µï¼Œç¼ºå¤±ç›´æ¥æŠ¥é”™ã€‚

---

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### æµ‹è¯•åˆ†ç±»

**æ­£å¼æµ‹è¯•** (`tests/toymodel/`)ï¼š
- å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- ä½¿ç”¨pytestæ¡†æ¶
- æäº¤åˆ°gitç‰ˆæœ¬æ§åˆ¶
- æœ€ä½è¦†ç›–ç‡è¦æ±‚: æ ¸å¿ƒç»„ä»¶ â‰¥ 80%, é€‚é…å™¨ â‰¥ 90%

**ä¸´æ—¶æµ‹è¯•** (`tmp/`)ï¼š
- å¿«é€ŸéªŒè¯å’Œè°ƒè¯•
- ç”¨å®Œç«‹å³åˆ é™¤
- ä¸æäº¤åˆ°git (å·²åœ¨.gitignore)
- ç”Ÿå‘½å‘¨æœŸ â‰¤ 1å¤©

### ä¸´æ—¶æµ‹è¯•ä½¿ç”¨ç¤ºä¾‹

```bash
# 1. åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
cat > tmp/test_feature.py << 'EOF'
from toymodel.environment import QueueEnvironment
env = QueueEnvironment(...)
# Quick validation code
print("âœ“ Test passed")
EOF

# 2. è¿è¡Œæµ‹è¯•
python tmp/test_feature.py

# 3. ç«‹å³åˆ é™¤
rm tmp/test_feature.py
```

### å…³é”®æµ‹è¯•ç‚¹
1. **ç¯å¢ƒæµ‹è¯•**: reset/stepæ­£ç¡®æ€§ï¼Œé˜Ÿåˆ—ç¨³å®šæ€§
2. **çŠ¶æ€æµ‹è¯•**: ç»´åº¦æ­£ç¡®æ€§ï¼Œæ•°å€¼èŒƒå›´åˆç†æ€§
3. **å¥–åŠ±æµ‹è¯•**: è·¯ç”±æ­£ç¡®æ€§å½±å“å¥–åŠ±
4. **é›†æˆæµ‹è¯•**: PPOè®­ç»ƒæ”¶æ•›ï¼Œæ¥è¿‘Oracleæ€§èƒ½

```python
# tests/toymodel/test_integration.py (æ­£å¼æµ‹è¯•)

def test_ppo_vs_oracle():
    """éªŒè¯PPOæ¥è¿‘Oracleæ€§èƒ½."""
    # è®­ç»ƒPPO
    ppo_metrics = train_and_eval_ppo(steps=50000)

    # è¯„ä¼°Oracle
    oracle_metrics = eval_oracle()

    # éªŒè¯æ€§èƒ½ (10%å®¹å¿åº¦)
    assert ppo_metrics["mean_latency"] <= oracle_metrics["mean_latency"] * 1.1
    assert ppo_metrics["routing_accuracy"] >= 0.9
```

---

## ğŸ“Š ç›‘æ§è§„èŒƒ

### TensorBoardæŒ‡æ ‡
- `train/reward`: æ¯æ­¥å¥–åŠ±
- `train/policy_loss`: Actor loss
- `train/value_loss`: Critic loss
- `train/entropy`: ç­–ç•¥ç†µ
- `eval/routing_accuracy`: è·¯ç”±å‡†ç¡®ç‡
- `eval/mean_latency`: å¹³å‡å»¶è¿Ÿ
- `eval/p99_latency`: P99å»¶è¿Ÿ

### CSVå¯¼å‡º
- æ¯ä¸ªcheckpointä¿å­˜æŒ‡æ ‡CSVåˆ° `outputs/toymodel/metrics/`
- åŒ…å«: step, reward, latency, routing_accuracy, throughput

---

## ğŸ” ä»£ç å®¡æŸ¥Checklist

æäº¤å‰å¿…æ£€:
- [ ] ä»£ç åœ¨ `toymodel/` ç›®å½•ï¼Œæœªæ··å…¥ `vidur/` æˆ– `src/`
- [ ] æ— ç¦ç”¨å‘½åå‰ç¼€ (`enhanced_*`, `*_v2` ç­‰)
- [ ] é…ç½®å‚æ•°ç»è¿‡æ˜¾å¼éªŒè¯ï¼Œæ— fallback
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] é›†æˆæµ‹è¯•é€šè¿‡ (PPOæ”¶æ•›)
- [ ] TensorBoardæŒ‡æ ‡æ­£å¸¸è®°å½•

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- **æŠ€æœ¯æ–¹æ¡ˆ**: `docs/toymodel_ppo_routing_design.md` - è¯¦ç»†è®¾è®¡æ–‡æ¡£
- **å®ç°æŒ‡å—**: `docs/toymodel_implementation.md` - å¼€å‘æ­¥éª¤
- **ä¸»è§„èŒƒ**: `.claude/CLAUDE.md` - é¡¹ç›®é€šç”¨è§„èŒƒ

---

**è§„èŒƒç‰ˆæœ¬**: v1.0
**ç”Ÿæ•ˆæ—¥æœŸ**: 2025-10-01
**å¿…è¯»**: å¼€å‘Toy Modelç›¸å…³åŠŸèƒ½å‰å¿…é¡»é˜…è¯»æœ¬æ–‡æ¡£
