# Phase 3.2: DA-Based Hard Problem Filtering - Results

**Date**: 2026-01-17
**Status**: COMPLETED (No Improvement Observed)
**Prior**: [2026-01-16_phase3_difficulty_implementation.md](2026-01-16_phase3_difficulty_implementation.md)

---

## Background

Phase 3+ difficulty-stratified benchmarks showed unexpected pattern:

| Difficulty | RR@5 | DA | Analysis |
|------------|------|-----|----------|
| Easy | 87% | 64.6% | Normal |
| **Medium** | **97%** | **31.4%** | High repair, low diagnosis |
| Difficult | 90% | 79.4% | Normal |

**Key Insight**: Medium tier has lowest DA (31.4%) but highest RR@5 (97%). The model "fixes" problems without truly diagnosing - these are genuinely hard problems.

## Approach

Filter problems by per-problem Diagnosis Accuracy (DA < 0.5) to create a benchmark of problems where:
- Model can repair (RR = 100%)
- Model cannot diagnose (DA < 50%)

This provides clear improvement space for DGRO training.

## Implementation

### 1. DA Filtering

From `sft_medium_eval` (200 problems evaluated):

```
Total problems: 200
Mean DA: 37.9%
Median DA: 33.3%

DA < 0.5: 150 problems (75.0%)
```

### 2. Filtered Benchmark Created

**Location**: `data/benchmarks/or_debug_bench_hard_da/`

| Metric | Value |
|--------|-------|
| Problems | 450 |
| Source | or_debug_bench_medium |
| DA threshold | < 0.5 |
| Error types | All Type D |

### 3. DGRO Training

Configuration:
- β_kl: 0.001
- β_reward: 0.5 (from Phase 3+)
- Efficiency reward: ENABLED
- Num generations: 4
- Training epochs: 3

Training Results:
- **Total Steps**: 225
- **Mean Reward**: -49.97
- **Reward Std**: 1.14 (very low variance)
- **Duration**: ~3 hours

### 4. Evaluation Results

| Metric | SFT Baseline | DGRO Actual | Target | Status |
|--------|--------------|-------------|--------|--------|
| RR | 100% | 100% | ≥95% | ✅ Maintained |
| RR@5 | 100% | 100% | ≥95% | ✅ Maintained |
| RR@10 | 100% | 100% | ≥95% | ✅ Maintained |
| **DA** | **27%** | **27%** | >40% | ❌ No improvement |
| Avg Steps | 1.5 | 1.4 | ≤2.0 | ✅ Maintained |

## Analysis

**DGRO training produced identical results to SFT baseline**:
- DA unchanged at 27%
- No improvement despite targeted training on hard problems

**Root Cause**:
1. SFT model already near-optimal for repair (100% RR)
2. Low reward variance (std=1.14) means minimal gradient signal
3. Model's repair strategy is fixed at ~1.5 steps
4. DA improvement requires fundamentally different exploration strategies

**Comparison with Phase 3 GRPO**:
- Same pattern observed: GRPO/DGRO cannot improve when SFT is already optimal
- Reward variance is too low for RL to provide meaningful gradients

## Conclusions

1. **DA-based filtering successfully identified hard problems** - 450 problems where model repairs but doesn't diagnose
2. **DGRO training completed but showed no improvement** - Same result as Phase 3 GRPO
3. **Fundamental limitation**: When SFT achieves 100% repair rate, RL has no room to improve
4. **DA improvement may require**:
   - Explicit diagnosis reward component
   - Curriculum learning (start with easier problems)
   - Different exploration strategies (MCTS, multi-turn)
   - Larger model capacity or different architecture

## Files Created

| File | Description |
|------|-------------|
| `data/benchmarks/or_debug_bench_hard_da/` | Filtered benchmark (450 problems) |
| `/data/dgro_hard_da_output/` | DGRO adapter output |
| `/data/qwen3_dgro_hard_da_merged/` | Merged DGRO model |
| `outputs/experiments/2026-01-17/dgro_hard_da_eval/` | Evaluation results |

## Recommendations for Future Work

1. **Explicit Diagnosis Reward**: Add separate reward for correct IIS identification
2. **Curriculum Learning**: Progressive difficulty increase
3. **Monte Carlo Tree Search**: Strategic exploration during training
4. **Process Supervision**: Reward intermediate diagnostic steps

---

*Implementation: Qwen3-8B, Training Framework: TRL 0.24.0*
*Completed: 2026-01-17*
