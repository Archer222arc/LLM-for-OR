# SFT vs GRPO Model Comparison

## Overview

- **SFT Model**: /data/qwen3_or_debug_merged
- **GRPO Model**: /data/qwen3_or_debug_grpo_merged
- **Dataset**: data/benchmarks/or_debug_bench_full
- **Samples**: 200

## Main Results

| Metric | SFT | GRPO | Delta |
|--------|-----|------|-------|
| Recovery Rate | 100.0% | 100.0% | +0.0% |
| RR@5 | 83.0% | 83.0% | +0.0% |
| RR@10 | 99.0% | 99.0% | +0.0% |
| RR@15 | 100.0% | 100.0% | +0.0% |
| RR@20 | 100.0% | 100.0% | +0.0% |
| Avg Steps | 3.25 | 3.25 | +0.00 |
| Diagnosis Accuracy | 80.0% | 80.0% | +0.0% |
| Token Efficiency | 0.0% | 0.0% | +0.0% |

## Interpretation

**Finding**: SFT and GRPO models show similar performance.

This result is expected given that GRPO training showed zero reward variance,
indicating the SFT model was already well-optimized for this task.

**Implication**: For this domain, SFT alone achieves strong performance.
GRPO may benefit from:
- Higher temperature during training for diversity
- Larger, more diverse training dataset
- Different reward signal design

## Per-Problem Comparison

| Problem Type | SFT RR | GRPO RR |
|--------------|--------|---------|
| Type A | 100.0% | 100.0% |

---

Generated by `scripts/evaluation/compare_models.py`