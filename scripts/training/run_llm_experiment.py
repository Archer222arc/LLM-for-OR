#!/usr/bin/env python3
"""
LLMå®éªŒè¿è¡Œè„šæœ¬

Research Direction: Direction A (OR-Debug-Bench)
Documentation: docs/directions/A_OR_Debug_Bench/

åŠŸèƒ½ï¼š
    - åŠ è½½å®éªŒé…ç½®ï¼ˆYAMLï¼‰
    - åŠ è½½æ•°æ®é›†ï¼ˆJSONï¼‰
    - åˆå§‹åŒ–å„ç±»Agentï¼ˆLLM + Baselineï¼‰
    - è¿è¡ŒBenchmarkRunnerè¯„ä¼°ï¼ˆæ”¯æŒå¹¶è¡Œæ¨¡å¼ï¼‰
    - ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼ˆMarkdown + JSONï¼‰
    - ä¿å­˜å®éªŒè½¨è¿¹å’Œæ—¥å¿—

å¹¶è¡Œæ¨¡å¼:
    - ä½¿ç”¨ProcessPoolExecutorå®ç°Agentçº§åˆ«å¹¶è¡Œ
    - æ¯ä¸ªAgentåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼Œé¿å…Gurobiçº¿ç¨‹å®‰å…¨é—®é¢˜
    - é¢„æœŸåŠ é€Ÿ: ~10x (12 Agent Ã— 20 Problem ä»30åˆ†é’Ÿåˆ°3åˆ†é’Ÿ)

Usage:
    # è¿è¡Œå®Œæ•´å®éªŒï¼ˆé¡ºåºæ¨¡å¼ï¼‰
    python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml

    # å¹¶è¡Œæ¨¡å¼ï¼ˆæ¨èï¼‰
    python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --parallel --max-workers 4

    # åªè¿è¡ŒæŒ‡å®šagent
    python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --agents gpt4,heuristic

    # é™åˆ¶é—®é¢˜æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --limit 2 --parallel

    # éªŒè¯é…ç½®æ–‡ä»¶
    python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --validate-only

Dependencies:
    - src.agents: LLMAgent, HeuristicAgent, RandomAgent, GreedyDropAgent, DoNothingAgent
    - src.evaluation: BenchmarkRunner, BenchmarkConfig, BenchmarkProblem
    - src.solvers: GurobiSolver
    - src.environments: SolverDebugEnv

Created: 2026-01-11
Updated: 2026-01-11 (Added parallel mode)
"""

import argparse
import json
import os
import sys
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.agents import (
    LLMAgent, HeuristicAgent, RandomAgent,
    GreedyDropAgent, DoNothingAgent, BaseAgent
)
from src.evaluation import BenchmarkRunner, BenchmarkConfig, BenchmarkProblem
from src.solvers import GurobiSolver
from src.environments import SolverDebugEnv


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½YAMLé…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸

    Raises:
        FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
        yaml.YAMLError: YAMLæ ¼å¼é”™è¯¯
    """
    config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    print(f"âœ“ æˆåŠŸåŠ è½½é…ç½®: {config_path}")
    print(f"  å®éªŒåç§°: {config['experiment']['name']}")
    print(f"  æè¿°: {config['experiment']['description']}")

    return config


def load_dataset_from_json(dataset_path: str, limit: Optional[int] = None) -> List[BenchmarkProblem]:
    """
    ä»JSONåŠ è½½æ•°æ®é›†ï¼Œåˆ›å»ºBenchmarkProblemåˆ—è¡¨

    Args:
        dataset_path: æ•°æ®é›†JSONè·¯å¾„
        limit: é™åˆ¶åŠ è½½çš„é—®é¢˜æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰

    Returns:
        BenchmarkProblemåˆ—è¡¨

    Example:
        >>> problems = load_dataset_from_json("data/synthetic/debug_bench_v1/dataset.json")
        >>> print(f"åŠ è½½ {len(problems)} ä¸ªé—®é¢˜")
    """
    dataset_path = Path(dataset_path)

    if not dataset_path.exists():
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")

    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"\nâœ“ æˆåŠŸåŠ è½½æ•°æ®é›†: {dataset['dataset_name']}")
    print(f"  åˆ›å»ºæ—¶é—´: {dataset['created_at']}")
    print(f"  é—®é¢˜æ€»æ•°: {dataset['num_problems']}")

    problems = []
    problem_list = dataset['problems'][:limit] if limit else dataset['problems']

    for i, p in enumerate(problem_list):
        try:
            # åŠ è½½MPSæ¨¡å‹
            model_path = Path(p['model_file'])
            if not model_path.is_absolute():
                # ç›¸å¯¹äºæ•°æ®é›†æ–‡ä»¶çš„è·¯å¾„
                model_path = dataset_path.parent / model_path

            if not model_path.exists():
                print(f"  âš  è·³è¿‡é—®é¢˜ {p['problem_id']}: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
                continue

            solver = GurobiSolver.from_file(str(model_path))

            # åˆ›å»ºç¯å¢ƒ
            env = SolverDebugEnv(
                solver,
                problem_nl=p.get('problem_nl', ''),
                max_steps=50
            )

            # åˆ›å»ºBenchmarkProblem
            problem = BenchmarkProblem(
                problem_id=p['problem_id'],
                env=env,
                ground_truth_fix=p.get('ground_truth_fix'),
                ground_truth_iis=p.get('iis_constraints', []),
                metadata={
                    'error_type': p.get('error_type'),
                    'error_description': p.get('error_description'),
                    'n_variables': p.get('n_variables'),
                    'n_constraints': p.get('n_constraints'),
                    'iis_size': p.get('iis_size')
                }
            )

            problems.append(problem)

            if (i + 1) % 10 == 0:
                print(f"  å·²åŠ è½½ {i + 1}/{len(problem_list)} ä¸ªé—®é¢˜...")

        except Exception as e:
            print(f"  âš  åŠ è½½é—®é¢˜ {p['problem_id']} å¤±è´¥: {e}")
            continue

    print(f"âœ“ æˆåŠŸåŠ è½½ {len(problems)} ä¸ªé—®é¢˜\n")

    return problems


def create_agent_from_config(agent_config: Dict[str, Any]) -> BaseAgent:
    """
    ä»é…ç½®åˆ›å»ºAgentå®ä¾‹

    Args:
        agent_config: Agenté…ç½®å­—å…¸

    Returns:
        Agentå®ä¾‹

    Supported agent types:
        - llm: LLMAgent (OpenAI, Anthropic, etc.)
        - baseline: HeuristicAgent, RandomAgent, GreedyDropAgent, DoNothingAgent

    Example:
        >>> config = {
        ...     "name": "gpt4",
        ...     "type": "llm",
        ...     "model": "gpt-4",
        ...     "provider": "openai",
        ...     "temperature": 0.0
        ... }
        >>> agent = create_agent_from_config(config)
    """
    agent_type = agent_config['type']
    agent_name = agent_config['name']

    if agent_type == 'llm':
        # åˆ›å»ºLLM Agent
        provider = agent_config['provider']

        # Base parameters for all LLM providers
        llm_params = {
            'model': agent_config['model'],
            'provider': provider,
            'temperature': agent_config.get('temperature', 0.0),
            'max_retries': agent_config.get('max_retries', 3),
            'name': agent_name
        }

        # Add Azure OpenAI specific parameters
        if provider == "azure_openai":
            llm_params['azure_endpoint'] = agent_config.get('azure_endpoint')
            llm_params['api_version'] = agent_config.get('api_version', '2024-10-21')
            llm_params['azure_deployment'] = agent_config.get('azure_deployment')

        agent = LLMAgent(**llm_params)
        print(f"  âœ“ åˆ›å»ºLLM Agent: {agent_name} ({agent_config['model']})")

    elif agent_type == 'baseline':
        # åˆ›å»ºBaseline Agent
        cls_name = agent_config['class']

        if cls_name == 'HeuristicAgent':
            agent = HeuristicAgent(name=agent_name)
        elif cls_name == 'RandomAgent':
            seed = agent_config.get('seed', 42)
            agent = RandomAgent(seed=seed, name=agent_name)
        elif cls_name == 'GreedyDropAgent':
            agent = GreedyDropAgent(name=agent_name)
        elif cls_name == 'DoNothingAgent':
            agent = DoNothingAgent(name=agent_name)
        else:
            raise ValueError(f"æœªçŸ¥çš„Baseline Agentç±»å‹: {cls_name}")

        print(f"  âœ“ åˆ›å»ºBaseline Agent: {agent_name} ({cls_name})")

    else:
        raise ValueError(f"æœªçŸ¥çš„Agentç±»å‹: {agent_type}")

    return agent


def generate_markdown_report(
    comparison: Dict[str, Any],
    config: Dict[str, Any],
    output_path: Path
):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š

    Args:
        comparison: BenchmarkRunner.compare_agents()è¿”å›çš„ç»“æœ
        config: å®éªŒé…ç½®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    lines = []

    # åˆ¤æ–­comparisonç»“æ„ - å¯èƒ½æ˜¯ {agent: metrics} æˆ– {results: {...}, summary: {...}}
    if 'results' in comparison:
        results = comparison['results']
        n_problems = comparison.get('summary', {}).get('n_problems', 'N/A')
    else:
        # ç›´æ¥æ˜¯ {agent_name: metrics} æ ¼å¼
        results = comparison
        # ä»ç¬¬ä¸€ä¸ªagentçš„n_episodesæ¨æ–­é—®é¢˜æ•°
        first_metrics = next(iter(results.values()), {})
        n_problems = first_metrics.get('n_episodes', 'N/A')

    # æ ‡é¢˜å’Œå…ƒæ•°æ®
    lines.append(f"# {config['experiment']['name']}")
    lines.append(f"\n**æè¿°**: {config['experiment']['description']}")
    lines.append(f"\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"\n**æ•°æ®é›†**: {config['benchmark']['dataset']}")
    lines.append(f"\n**é—®é¢˜æ•°é‡**: {n_problems}")
    lines.append("\n---\n")

    # æ€»ä½“å¯¹æ¯”è¡¨æ ¼
    lines.append("## Agentæ€§èƒ½å¯¹æ¯”\n")

    # æ£€æŸ¥æ˜¯å¦æœ‰DAæŒ‡æ ‡
    has_da = any('diagnosis_accuracy' in m for m in results.values())

    if has_da:
        lines.append("| Agent | Recovery Rate | Avg Steps | Diag Accuracy | Diag Precision | Step Efficiency |")
        lines.append("|-------|---------------|-----------|---------------|----------------|-----------------|")
    else:
        lines.append("| Agent | Recovery Rate | Avg Steps | Avg Reward | Success Rate | Step Efficiency |")
        lines.append("|-------|---------------|-----------|------------|--------------|-----------------|")

    for agent_name, metrics in results.items():
        if has_da:
            lines.append(
                f"| {agent_name} | "
                f"{metrics.get('recovery_rate', 0.0):.1%} | "
                f"{metrics.get('avg_steps', 0.0):.2f} | "
                f"{metrics.get('diagnosis_accuracy', 0.0):.1%} | "
                f"{metrics.get('diagnosis_precision', 0.0):.1%} | "
                f"{metrics.get('step_efficiency', 0.0):.2f} |"
            )
        else:
            lines.append(
                f"| {agent_name} | "
                f"{metrics.get('recovery_rate', 0.0):.1%} | "
                f"{metrics.get('avg_steps', 0.0):.2f} | "
                f"{metrics.get('avg_reward', 0.0):.2f} | "
                f"{metrics.get('success_rate', 0.0):.1%} | "
                f"{metrics.get('step_efficiency', 0.0):.2f} |"
            )

    lines.append("\n---\n")

    # è¯¦ç»†æŒ‡æ ‡
    lines.append("## è¯¦ç»†æŒ‡æ ‡\n")
    for agent_name, metrics in results.items():
        lines.append(f"### {agent_name}\n")
        for metric_name, value in metrics.items():
            if isinstance(value, float):
                lines.append(f"- **{metric_name}**: {value:.4f}")
            else:
                lines.append(f"- **{metric_name}**: {value}")
        lines.append("\n")

    # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'by_error_type' in comparison:
        lines.append("## æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡\n")
        for error_type, stats in comparison['by_error_type'].items():
            lines.append(f"### Type {error_type}\n")
            lines.append(f"- é—®é¢˜æ•°é‡: {stats['count']}")
            lines.append(f"- å¹³å‡æ¢å¤ç‡: {stats.get('avg_recovery_rate', 0.0):.1%}\n")

    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def run_experiment(
    config: Dict[str, Any],
    selected_agents: Optional[List[str]] = None,
    limit: Optional[int] = None,
    parallel: bool = False,
    max_workers: int = 4
):
    """
    è¿è¡Œå®Œæ•´å®éªŒ

    Args:
        config: å®éªŒé…ç½®å­—å…¸
        selected_agents: ä»…è¿è¡ŒæŒ‡å®šçš„agentï¼ˆNoneè¡¨ç¤ºè¿è¡Œå…¨éƒ¨ï¼‰
        limit: é™åˆ¶é—®é¢˜æ•°é‡
        parallel: æ˜¯å¦å¹¶è¡Œè¿è¡Œï¼ˆæ¯ä¸ªAgentä¸€ä¸ªè¿›ç¨‹ï¼‰
        max_workers: å¹¶è¡Œæœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    """
    print("\n" + "="*60)
    print("å¼€å§‹è¿è¡ŒLLMå®éªŒ" + (" [å¹¶è¡Œæ¨¡å¼]" if parallel else " [é¡ºåºæ¨¡å¼]"))
    print("="*60 + "\n")

    dataset_path = config['benchmark']['dataset']

    if parallel:
        # å¹¶è¡Œæ¨¡å¼ï¼šç›´æ¥ä¼ é…ç½®ç»™workerè¿›ç¨‹
        print("[1/3] å‡†å¤‡Agenté…ç½®...")

        agent_configs = []
        for agent_config in config['agents']:
            agent_name = agent_config['name']
            if selected_agents and agent_name not in selected_agents:
                print(f"  âŠ˜ è·³è¿‡ Agent: {agent_name}")
                continue
            agent_configs.append(agent_config)
            print(f"  âœ“ åŠ å…¥Agent: {agent_name}")

        if len(agent_configs) == 0:
            print("âŒ æœªé€‰æ‹©ä»»ä½•Agentï¼Œå®éªŒç»ˆæ­¢")
            return

        print(f"\nâœ“ å…± {len(agent_configs)} ä¸ªAgentå°†å¹¶è¡Œè¿è¡Œ\n")

        # åˆ›å»ºBenchmarkRunnerå¹¶è¿è¡Œå¹¶è¡Œè¯„ä¼°
        print("[2/3] å¹¶è¡Œè¿è¡Œè¯„ä¼°...")

        runner = BenchmarkRunner(
            config=BenchmarkConfig(
                max_steps=config['benchmark']['max_steps'],
                n_episodes=config['benchmark'].get('n_episodes', 1),
                verbose=config['benchmark'].get('verbose', True)
            )
        )

        try:
            comparison = runner.compare_agents_parallel(
                dataset_path=dataset_path,
                agent_configs=agent_configs,
                max_workers=max_workers,
                limit=limit,
            )
        except Exception as e:
            print(f"âŒ å¹¶è¡Œè¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

        # ç”ŸæˆæŠ¥å‘Š
        print("\n[3/3] ç”ŸæˆæŠ¥å‘Š...")

    else:
        # é¡ºåºæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
        # 1. åŠ è½½æ•°æ®é›†
        print("[1/4] åŠ è½½æ•°æ®é›†...")
        problems = load_dataset_from_json(dataset_path, limit=limit)

        if len(problems) == 0:
            print("âŒ æœªèƒ½åŠ è½½ä»»ä½•é—®é¢˜ï¼Œå®éªŒç»ˆæ­¢")
            return

        # 2. åˆ›å»ºAgents
        print("[2/4] åˆ›å»ºAgents...")
        agents = []

        for agent_config in config['agents']:
            agent_name = agent_config['name']

            # å¦‚æœæŒ‡å®šäº†selected_agentsï¼Œåªåˆ›å»ºé€‰ä¸­çš„
            if selected_agents and agent_name not in selected_agents:
                print(f"  âŠ˜ è·³è¿‡ Agent: {agent_name}")
                continue

            try:
                agent = create_agent_from_config(agent_config)
                agents.append(agent)
            except Exception as e:
                print(f"  âš  åˆ›å»ºAgent {agent_name} å¤±è´¥: {e}")
                continue

        if len(agents) == 0:
            print("âŒ æœªèƒ½åˆ›å»ºä»»ä½•Agentï¼Œå®éªŒç»ˆæ­¢")
            return

        print(f"\nâœ“ å…±åˆ›å»º {len(agents)} ä¸ªAgent\n")

        # 3. åˆ›å»ºBenchmarkRunnerå¹¶è¿è¡Œ
        print("[3/4] è¿è¡Œè¯„ä¼°...")

        runner = BenchmarkRunner(
            config=BenchmarkConfig(
                max_steps=config['benchmark']['max_steps'],
                n_episodes=config['benchmark'].get('n_episodes', 1),
                verbose=config['benchmark'].get('verbose', True)
            )
        )

        try:
            comparison = runner.compare_agents(problems, agents)
        except Exception as e:
            print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

        # 4. ç”ŸæˆæŠ¥å‘Š
        print("\n[4/4] ç”ŸæˆæŠ¥å‘Š...")

    output_dir = Path(config['experiment']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜JSONç»“æœ
    json_path = output_dir / 'results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, indent=2, ensure_ascii=False)
    print(f"âœ“ JSONç»“æœå·²ä¿å­˜: {json_path}")

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    if 'markdown' in config['evaluation'].get('report_format', ['markdown']):
        md_path = output_dir / 'report.md'
        generate_markdown_report(comparison, config, md_path)

    # ä¿å­˜è½¨è¿¹ï¼ˆå¦‚æœé…ç½®ï¼‰
    if config['evaluation'].get('save_trajectories', False):
        trajectories_dir = output_dir / 'trajectories'
        trajectories_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ è½¨è¿¹ä¿å­˜ç›®å½•: {trajectories_dir}")

    print("\n" + "="*60)
    print("å®éªŒå®Œæˆï¼")
    print("="*60 + "\n")

    # æ‰“å°ç®€è¦ç»“æœ
    print("ğŸ“Š å®éªŒç»“æœæ¦‚è§ˆ:\n")
    # åˆ¤æ–­comparisonç»“æ„
    results = comparison.get('results', comparison) if isinstance(comparison, dict) else comparison
    for agent_name, metrics in results.items():
        print(f"{agent_name}:")
        print(f"  Recovery Rate: {metrics.get('recovery_rate', 0.0):.1%}")
        print(f"  Avg Steps: {metrics.get('avg_steps', 0.0):.2f}")
        print(f"  Success Rate: {metrics.get('success_rate', 0.0):.1%}\n")


def validate_config(config: Dict[str, Any]) -> bool:
    """
    éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        æ˜¯å¦é€šè¿‡éªŒè¯
    """
    print("\néªŒè¯é…ç½®æ–‡ä»¶...\n")

    required_keys = {
        'experiment': ['name', 'description', 'output_dir'],
        'benchmark': ['dataset', 'max_steps'],
        'agents': [],
        'evaluation': ['metrics']
    }

    valid = True

    # æ£€æŸ¥é¡¶å±‚é”®
    for key in required_keys.keys():
        if key not in config:
            print(f"âŒ ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {key}")
            valid = False
        elif key != 'agents':
            for subkey in required_keys[key]:
                if subkey not in config[key]:
                    print(f"âŒ ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: {key}.{subkey}")
                    valid = False

    # æ£€æŸ¥agentsé…ç½®
    if 'agents' in config:
        if not isinstance(config['agents'], list) or len(config['agents']) == 0:
            print("âŒ agentså¿…é¡»æ˜¯éç©ºåˆ—è¡¨")
            valid = False
        else:
            for i, agent in enumerate(config['agents']):
                if 'name' not in agent:
                    print(f"âŒ agents[{i}]ç¼ºå°‘nameå­—æ®µ")
                    valid = False
                if 'type' not in agent:
                    print(f"âŒ agents[{i}]ç¼ºå°‘typeå­—æ®µ")
                    valid = False

    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    if 'benchmark' in config and 'dataset' in config['benchmark']:
        dataset_path = Path(config['benchmark']['dataset'])
        if not dataset_path.exists():
            print(f"âš  æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
            print("  ï¼ˆè¿è¡Œæ—¶ä¼šå¤±è´¥ï¼Œä½†é…ç½®æ ¼å¼æ­£ç¡®ï¼‰")

    if valid:
        print("âœ“ é…ç½®éªŒè¯é€šè¿‡\n")
    else:
        print("\nâŒ é…ç½®éªŒè¯å¤±è´¥\n")

    return valid


def main():
    parser = argparse.ArgumentParser(
        description="è¿è¡ŒLLMå®éªŒè¯„ä¼°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´å®éªŒï¼ˆé¡ºåºæ¨¡å¼ï¼‰
  python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml

  # å¹¶è¡Œæ¨¡å¼ï¼ˆæ¨èï¼Œ~10xåŠ é€Ÿï¼‰
  python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --parallel --max-workers 4

  # åªè¿è¡ŒæŒ‡å®šagent
  python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --agents gpt4,heuristic

  # é™åˆ¶é—®é¢˜æ•°é‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
  python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --limit 2 --parallel

  # ä»…éªŒè¯é…ç½®
  python scripts/run_llm_experiment.py --config configs/experiments/llm_eval.yaml --validate-only
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='å®éªŒé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæ ¼å¼ï¼‰'
    )

    parser.add_argument(
        '--agents',
        type=str,
        default=None,
        help='ä»…è¿è¡ŒæŒ‡å®šçš„agentsï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œä¾‹å¦‚: gpt4,claude3-sonnet'
    )

    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='é™åˆ¶é—®é¢˜æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰'
    )

    parser.add_argument(
        '--validate-only',
        action='store_true',
        help='ä»…éªŒè¯é…ç½®æ–‡ä»¶ï¼Œä¸è¿è¡Œå®éªŒ'
    )

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='å¯ç”¨å¹¶è¡Œæ¨¡å¼ï¼ˆæ¯ä¸ªAgentåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        default=4,
        help='å¹¶è¡Œæ¨¡å¼æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰'
    )

    args = parser.parse_args()

    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)

        # éªŒè¯é…ç½®
        if args.validate_only:
            validate_config(config)
            return

        # è§£æselected_agents
        selected_agents = None
        if args.agents:
            selected_agents = [name.strip() for name in args.agents.split(',')]
            print(f"\nä»…è¿è¡Œä»¥ä¸‹agents: {', '.join(selected_agents)}\n")

        # å¹¶è¡Œæ¨¡å¼æç¤º
        if args.parallel:
            print(f"\nå¯ç”¨å¹¶è¡Œæ¨¡å¼ï¼Œmax_workers={args.max_workers}")
            print("æ³¨æ„: æ¯ä¸ªAgentå°†åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼Œé¿å…Gurobiçº¿ç¨‹å®‰å…¨é—®é¢˜\n")

        # è¿è¡Œå®éªŒ
        run_experiment(
            config,
            selected_agents=selected_agents,
            limit=args.limit,
            parallel=args.parallel,
            max_workers=args.max_workers
        )

    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}\n")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
